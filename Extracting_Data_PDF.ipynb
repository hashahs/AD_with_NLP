{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a09b5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from PyPDF2) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f819de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81b6858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PyPDF2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6b382ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 35\n",
      " \n",
      " \n",
      " Development  Plan for Greater Mumbai 2014‐2034                                                                                                                                                                                                                                                      \n",
      "Acknowledgements  \n",
      "The Consultant  wishes to thank the following  individuals  from the Municipal  Corporation  of \n",
      "Greater Mumbai for their invaluable  support, insights and contributions  towards ‘Working  Paper 1 \n",
      "– Preparation  of Base Map’ for the preparation  of the Development  Plan for Greater Mumbai \n",
      "2014‐34. \n",
      " Mr. Subodh Kumar, IAS, Municipal  Commissioner;  \n",
      " Mr. Rajeev Kuknoor, Chief Engineer Development  Plan; \n",
      " Mr. Sudhir Ghate, Deputy Chief Engineer Development  Plan; \n",
      " Mr. A.G. Marathe, Deputy Chief Engineer Development  Plan; \n",
      " Mr. R. Balachandran,  Executive  Engineer and Town Planning Officer, Development  Plan. \n",
      " Our gratitude  to the following  experts for their invaluable  insights and support: \n",
      " \n",
      "Mr. V.K Phatak, Former Chief Town Planner (MMRDA);  \n",
      " Mr. A.N Kale, Former Chief Engineer, (DP); \n",
      " Mr. A. S Jain Former Dy. Chief Engineer, (DP). \n",
      " We wish to especially  thank MCGM officers, Mr. Jagdish Talreja, Mr. Dinesh Naik, Mr. Hiren \n",
      "Daftardar,  Ms. Anita Naik for their continual  support since the\n",
      " beginning  of the project and their \n",
      "help towards familiarization  and data collection.  They have been instrumental  in helping to \n",
      "contact various MCGM departments  as well as in helping to establish contact with personnel  from \n",
      "other government  departments  and organizations.  Many thanks for the MCGM team, for \n",
      "deploying  personnel,  particularly  Mr. Prasad Gharat, on extensive  field visits that have helped in \n",
      "understanding  actual ground conditions.  \n",
      " \n",
      "We apologize  if we have inadvertently  omitted anyone to whom acknowledgement  is due. We hope \n",
      "and anticipate  the work's usefulness  for the intended purpose. \n",
      " \n"
     ]
    }
   ],
   "source": [
    "pdf= open(r\"C:\\Users\\Harsha\\Downloads\\file1pdf.pdf\",\"rb\")\n",
    "\n",
    "pdf_reader=PyPDF2.PdfReader(pdf)\n",
    "\n",
    "print(\"Number of pages:\",len(pdf_reader.pages))\n",
    "\n",
    "page=pdf_reader.pages[1]\n",
    "print(page.extract_text())\n",
    "\n",
    "pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "869a9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2, urllib, nltk\n",
    "from io import BytesIO\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23b4846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wFile=urllib.request.urlopen('http://www.udri.org/pdf/02%20working%20paper%201.pdf')\n",
    "pdfreader=PyPDF2.PdfReader(BytesIO(wFile.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e191bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageObj=pdfreader.pages[2]\n",
    "page2=pageObj.extract_text()\n",
    "\n",
    "punctutations=['(',')',';',':','[',']',',','...','.']\n",
    "tokens=word_tokenize(page2)\n",
    "stop_words=stopwords.words('english')\n",
    "keywords=[word for word in tokens if not word in stop_words and not word in punctutations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b710c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '2014‐2034',\n",
       " 'Table',\n",
       " 'Contents',\n",
       " 'The',\n",
       " 'Consultant',\n",
       " 'wishes',\n",
       " 'thank',\n",
       " 'following',\n",
       " 'individuals',\n",
       " 'Municipal',\n",
       " 'Corporation',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " 'invaluable',\n",
       " 'support',\n",
       " 'insights',\n",
       " 'contributions',\n",
       " 'towards',\n",
       " '‘',\n",
       " 'Working',\n",
       " 'Paper',\n",
       " '1',\n",
       " '–',\n",
       " 'Preparation',\n",
       " 'Base',\n",
       " 'Map',\n",
       " '’',\n",
       " 'preparation',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '2014‐34',\n",
       " '.............................................................................................................................',\n",
       " '..............',\n",
       " '3',\n",
       " 'Our',\n",
       " 'gratitude',\n",
       " 'following',\n",
       " 'experts',\n",
       " 'invaluable',\n",
       " 'insights',\n",
       " 'support',\n",
       " '............................',\n",
       " '3',\n",
       " 'We',\n",
       " 'wish',\n",
       " 'especially',\n",
       " 'thank',\n",
       " 'MCGM',\n",
       " 'officers',\n",
       " 'Mr.',\n",
       " 'Jagdish',\n",
       " 'Talreja',\n",
       " 'Mr.',\n",
       " 'Dinesh',\n",
       " 'Naik',\n",
       " 'Mr.',\n",
       " 'Hiren',\n",
       " 'Daftardar',\n",
       " 'Ms.',\n",
       " 'Anita',\n",
       " 'Naik',\n",
       " 'continual',\n",
       " 'support',\n",
       " 'since',\n",
       " 'beginning',\n",
       " 'project',\n",
       " 'help',\n",
       " 'towards',\n",
       " 'familiarization',\n",
       " 'data',\n",
       " 'collection',\n",
       " 'They',\n",
       " 'instrumental',\n",
       " 'helping',\n",
       " 'contact',\n",
       " 'various',\n",
       " 'MCGM',\n",
       " 'departments',\n",
       " 'well',\n",
       " 'helping',\n",
       " 'establish',\n",
       " 'contact',\n",
       " 'personnel',\n",
       " 'government',\n",
       " 'departments',\n",
       " 'organizations',\n",
       " 'Many',\n",
       " 'thanks',\n",
       " 'MCGM',\n",
       " 'team',\n",
       " 'deploying',\n",
       " 'personnel',\n",
       " 'particularly',\n",
       " 'Mr.',\n",
       " 'Prasad',\n",
       " 'Gharat',\n",
       " 'extensive',\n",
       " 'field',\n",
       " 'visits',\n",
       " 'helped',\n",
       " 'understanding',\n",
       " 'actual',\n",
       " 'ground',\n",
       " 'conditions',\n",
       " '........................................................................................',\n",
       " '3',\n",
       " 'BEST',\n",
       " '...............................................................................................................................',\n",
       " '.................',\n",
       " '5',\n",
       " 'Brihanmumbai',\n",
       " 'Electric',\n",
       " 'Supply',\n",
       " 'Transport',\n",
       " 'Undertaking',\n",
       " '..............................................................',\n",
       " '5',\n",
       " 'CIDCO',\n",
       " '...............................................................................................................................',\n",
       " '..............',\n",
       " '5',\n",
       " 'City',\n",
       " 'Industrial',\n",
       " 'Development',\n",
       " 'Corporation',\n",
       " '...............................................................................',\n",
       " '5',\n",
       " 'CTP',\n",
       " '...............................................................................................................................',\n",
       " '..................',\n",
       " '5',\n",
       " 'Comprehensive',\n",
       " 'Transportation',\n",
       " 'Plan',\n",
       " '...............................................................................................',\n",
       " '5',\n",
       " 'DP',\n",
       " '...............................................................................................................................',\n",
       " '....................',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " '..........................................................................................................................',\n",
       " '5',\n",
       " 'DPGM34',\n",
       " '...............................................................................................................................',\n",
       " '..........',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '2034',\n",
       " '.......................................................................................',\n",
       " '5',\n",
       " 'DCR',\n",
       " '...............................................................................................................................',\n",
       " '..................',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Control',\n",
       " 'Regulations',\n",
       " '...................................................................................................',\n",
       " '5',\n",
       " 'DGPS',\n",
       " '...........................................................................................................................',\n",
       " '....................',\n",
       " '5',\n",
       " 'Digital',\n",
       " 'Global',\n",
       " 'Positioning',\n",
       " 'System',\n",
       " '...................................................................................................',\n",
       " '5',\n",
       " 'DPGM',\n",
       " '...............................................................................................................................',\n",
       " '..............',\n",
       " '5',\n",
       " 'Development',\n",
       " 'Plan',\n",
       " 'Greater',\n",
       " 'Mumbai',\n",
       " '...........................................................................................',\n",
       " '5',\n",
       " 'ELU',\n",
       " '...............................................................................................................................',\n",
       " '..................',\n",
       " '5',\n",
       " 'Existing',\n",
       " 'Land',\n",
       " 'use',\n",
       " '.............................................................................................................................',\n",
       " '5',\n",
       " 'FSI',\n",
       " '...............................................................................................................................',\n",
       " '....................',\n",
       " '5',\n",
       " 'Floor',\n",
       " 'Space',\n",
       " 'Index',\n",
       " '............................................................................................................................',\n",
       " '5',\n",
       " 'GIS',\n",
       " '...............................................................................................................................',\n",
       " '...................',\n",
       " '5']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d57dda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.JagdishTalreja']\n",
      "['Mr.JagdishTalreja', 'Mr.DineshNaik']\n",
      "['Mr.JagdishTalreja', 'Mr.DineshNaik', 'Mr.HirenDaftardar']\n",
      "['Mr.JagdishTalreja', 'Mr.DineshNaik', 'Mr.HirenDaftardar', 'Ms.AnitaNaik']\n",
      "['Mr.JagdishTalreja', 'Mr.DineshNaik', 'Mr.HirenDaftardar', 'Ms.AnitaNaik', 'Mr.PrasadGharat']\n"
     ]
    }
   ],
   "source": [
    "name_list=list()\n",
    "check=['Mr.','Mrs.','Ms.']\n",
    "for idx, token in enumerate(tokens):\n",
    "    if token.startswith(tuple(check)) and idx <(len(tokens)-1):\n",
    "        name=token +tokens[idx+1]+''+tokens[idx+2]\n",
    "        name_list.append(name)\n",
    "        print(name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65deb50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45dea7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b287a127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docxNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "     -------------------------------------- 244.3/244.3 kB 7.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from python-docx) (4.12.2)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from python-docx) (4.9.1)\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.2\n"
     ]
    }
   ],
   "source": [
    "pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c58fee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9241c961",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=open(r\"C:\\Users\\Harsha\\Downloads\\customer_review_variation_1.docx\",\"rb\")\n",
    "document=docx.Document(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd8dbde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In  [1]:Out[1]:In  [2]  :\n",
      "# (v1)  Zfnport necessary  L ibrani esimport pandas as pd_v1 import numpy as np_v1 import reimport nltkfrom nltk.corpus import stopwordsfrom nltk.stem import WordNetLemmatizerfrom sklearn.model_selection import train_test_splitfrom sklearn.feature_extraction.text import TfidfVectorizer from sklearn.ensemble import RandomForestClassifierfrom sklearn.linear_model import LogisticRegression from sklearn.svm import SVCfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_! import matplotlib.pyplot as pltimport seaborn as sns# (v1)  NL TK Down Loads nltk.download(”stopwords\") nltk.download(”wordnet”)[nltk_data]  Downloading  package  stopwords  to[nltk_data]\t\tC:\\Users\\faree\\AppData\\Roaming\\nltk_data... [nltk_data]\tPackage stopwords is already up-to-date! [nltk_data] Downloading package wordnet to[nltk_data]\t\tC:\\Users\\faree\\AppData\\Roaming\\nltk_data... [nltk_data]\tPackage Wordnet is already up-to-date!True# (v1)  1.  Data Co L Lect i on -  Load the  datas etdata\tpd.read_csv(”synthetic_sentiment_dataset.csv\")In [3]: # 2.def_1def_1def_1def_1\n",
      "Data  Preproces  sing    Techniquesclean_text(text):“”\"Remove special characters, convert to lowercase.\"\"” text_v1= text.lower()return  re.sub(r”[’a-z\\s]\",  ””,  text)tokenize_text(text):““”Tokenize the text.\"\"” return text.split()remove_stopwords(tokens):“\"\"Remove common stopwords. ”””return [word for word in tokens if word not in stopwords.words(\"english”)]lemmatize_tokens(tokens):“””Lemmatize tokens to their base form. ””” lemmatizer_v1= WordNetLemmatizer()return [lemmatizer.lemmatize(word) for word in tokens]# (v1)  AppLy pneproces si ng yunc I i onsdata[”Cleaned_Text”]\tdata[”Review”].apply(clean_text) data[”Tokens“]\tdata[\"Cleaned_Text”].apply(tokenize_text) data[”Tokens_No_Stopwords\"]\tdata[\"Tokens”].apply(remove_stopwords)data[”Lemmatized_Tokens\"]\tdata[\"Tokens_No_Stopwords\"].apply(lemmatize_tokens  data[”Processed_Text\"]\tdata[”Lemmatized_Tokens”].apply(lambda  tokens:\t”.jo:In [4]:\n",
      "# (v1)  3. Vi suaL i z i ng Dataset and Key Stats print(”Full Dataset with Preprocessing Steps:”) print(data)Full Dataset with Preprocessing Steps:Review SentimentCleaned_Text  This  product  is  fine.\tneutralThis product is hate. negativeThis product is love. positiveThis product is okay.\tneutralThis product is poor. negative\n",
      "this product is fine this product is hate this product is love this product is okay this product is poorThis  product  is  fantastic.This product is satisfactory.This product is mediocre.This product is great.This  product  is  great.\n",
      "positive  this  product  is  fantastic  neutral  this  product  is  satisfactory  neutral  this  product  is  mediocre  positive        this  product  is  greatpositive        this  product  is  greatTokens[this, product, is, fine][this, product, is, hate][this, product, is, love][this, product, is, okay][this, product, is, poor]\n",
      "Tokens_No_Stopwords  \\  [product,  fine]  [product,  hate]  [product,  love]  [product,  okay]  [product,  poor][this, product, is, fantastic]\t[product, fantastic][this, product, is, satisfactory] [product, satisfactory][this, product, is, mediocre]\t[product, mediocre][this, product, is, great]\t[product, great][this, product, is, great]\t[product, great]Lemmatized_Tokens[product, fine][product,  hate][product, love][product,  okay][product, poor]\n",
      "Processed_Text product fine product hate product love product okay product poor[product,  fantastic]\tproduct  fantastic[product, satisfactory] product satisfactory[product, mediocre]\tproduct mediocre[product, great]\tproduct great[product, great]\tproduct great [1000 rows x 7 columns]In\t[5] :In [6]:In [8] :In [10]:\n",
      "# (v1)  5.  NodeL Deve Lopment & EvaL uat i onmodels\t(“Random Forest” : RandomForestClassifier(n_estimators_v1=100, random_state=42) “Logistic Regression” : LogisticRegression(random_state=42),“SVM” : SVC(kernel_v1=\"linear”, random_state=42)best_model_v1= None best_accuracy = 0print(\"Model Performance Evaluation: \")# (v1)  EvoLUote nodeLs Ond select the best one for\tmode1_name, model 1n mode1s.1tems(): model.fit(X_train_tfidf, y_train)y_pred_v1= model.predict(X_test_tfidf)# (v1)  Netn! cs caL cuL ationaccuracy_v1= accuracy_score(y_test, y_pred)precision\tprecision_score(y_test, y_pred, average_v1=\"weighted”) recall = recall_score(y_test, y_pred, average=”weighted\")f1_v1= f1_score(y_test, y_pred, average=\"weighted\")print(f\"\\n{model_name} Performance:\") print(f\"Accuracy: {accuracy * 100:.2f}%“) print(f\"Precision: {precision:.2f}”) print(f\"Recall: {recall:.2f}\")print(f\"F1-Score: {f1:.2f}”)ii Trach  I:he best: code Lif accuracy > best_accuracy: best_accuracy_v1= accuracy best_model = modelModel Performance Evaluation:Random  Forest  Performance:  Accuracy:  100.00%Precision: 1.00Recall: 1.00F1-Score: 1.00Logistic Regression Performance: Accuracy: 100.00%Precision:  1.00Recall: 1.00F1-Score:  1.00SVM Performance:Accuracy: 100.00%Precision:  1.00Recall: 1.00F1-Score:  1.00In [11]:Best Model: RandomForestClassifier(random_state_v1=42)In [12] :  # 7.  Di spL ay Dataset with Predicted Sentimentdata[”Predicted_Sentiment”]\tbest_model.predict(vectorizer.transform(data[“Pro# (v1)  FinaL Dataset  with predi c I i onsprint(”\\nFinal Dataset with Predicted Sentiments: ”) print(data)# (v1)  Save the  fi naL  dataset with pnedi ctions data.to_csv(\"final_processed_dataset_with_predictions.csv”, index=False) print(”Final processed dataset with predictions has been saved successfully! \")1/13/25,  1:58  PM\tUntitled83  -  Jupyter  NotebookFinal Dataset with Predicted Sentiments:Review SentimentThis product is fine.\tneutralThis  product  is  hate.  negativeThis product is love. positiveThis product is okay.\tneutralThis  product  is  poor.  negativeCleaned_Text  \\  this  product  is  fine  this  product  is  hate  this  product  is  love  this  product  is  okay  this  product  is  poorThis product is fantastic.This product is satisfactory.This product is mediocre.This product is great.This product is great.\n",
      "positive neutral neutral positive positive\n",
      "this product is fantastic this product is satisfactory this product is mediocre this product is greatthis product is great[this, product, is, fantastic][this, product, is, satisfactory][this,  product,  is,  mediocre][this,  product,  is,  great][this,  product,  is,  great]\n",
      "[product, fantastic] [product, satisfactory] [product, mediocre] [product, great] [product, great]Lemmatized_Tokens[product,  fine][product, hate][product, love][product, okay][product,  poor]\n",
      "Processed_Text  product  fine  product  hate  product  love  product  okay  product  poor\n",
      "Text_Length \\1212121212[product, fantastic]\tproduct fantastic\t17[product, satisfactory] product satisfactory\t20[product, mediocre]\tproduct mediocre\t16[product,  great]\tproduct  great\t13[product, great]\tproduct great\t13Predicted_Sentimentneutralnegativepositiveneutralnegativepositiveneutralneutralpositivepositive[1000 rows x 9 columns]Final processed dataset with predictions has been saved successfully!\n"
     ]
    }
   ],
   "source": [
    "docu=\"\"\n",
    "for para in document.paragraphs:\n",
    "    docu+=para.text\n",
    "\n",
    "print(docu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ee41fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The content of thr paragraph0is :In  [1]:\n",
      "\n",
      "The content of thr paragraph1is :Out[1]:\n",
      "\n",
      "The content of thr paragraph2is :In  [2]  :\n",
      "\n",
      "The content of thr paragraph3is :\n",
      "# (v1)  Zfnport necessary  L ibrani es\n",
      "\n",
      "The content of thr paragraph4is :import pandas as pd_v1 import numpy as np_v1 import re\n",
      "\n",
      "The content of thr paragraph5is :import nltk\n",
      "\n",
      "The content of thr paragraph6is :from nltk.corpus import stopwords\n",
      "\n",
      "The content of thr paragraph7is :from nltk.stem import WordNetLemmatizer\n",
      "\n",
      "The content of thr paragraph8is :from sklearn.model_selection import train_test_split\n",
      "\n",
      "The content of thr paragraph9is :from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "The content of thr paragraph10is :from sklearn.linear_model import LogisticRegression from sklearn.svm import SVC\n",
      "\n",
      "The content of thr paragraph11is :from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_! import matplotlib.pyplot as plt\n",
      "\n",
      "The content of thr paragraph12is :import seaborn as sns\n",
      "\n",
      "The content of thr paragraph13is :# (v1)  NL TK Down Loads nltk.download(”stopwords\") nltk.download(”wordnet”)\n",
      "\n",
      "The content of thr paragraph14is :[nltk_data]  Downloading  package  stopwords  to\n",
      "\n",
      "The content of thr paragraph15is :[nltk_data]\t\tC:\\Users\\faree\\AppData\\Roaming\\nltk_data... [nltk_data]\tPackage stopwords is already up-to-date! [nltk_data] Downloading package wordnet to\n",
      "\n",
      "The content of thr paragraph16is :[nltk_data]\t\tC:\\Users\\faree\\AppData\\Roaming\\nltk_data... [nltk_data]\tPackage Wordnet is already up-to-date!\n",
      "\n",
      "The content of thr paragraph17is :True\n",
      "\n",
      "The content of thr paragraph18is :# (v1)  1.  Data Co L Lect i on -  Load the  datas et\n",
      "\n",
      "The content of thr paragraph19is :data\tpd.read_csv(”synthetic_sentiment_dataset.csv\")\n",
      "\n",
      "The content of thr paragraph20is :In [3]: # 2.\n",
      "\n",
      "The content of thr paragraph21is :def_1\n",
      "\n",
      "The content of thr paragraph22is :def_1\n",
      "\n",
      "The content of thr paragraph23is :def_1\n",
      "\n",
      "The content of thr paragraph24is :def_1\n",
      "\n",
      "The content of thr paragraph25is :\n",
      "Data  Preproces  sing    Techniques\n",
      "\n",
      "The content of thr paragraph26is :clean_text(text):\n",
      "\n",
      "The content of thr paragraph27is :“”\"Remove special characters, convert to lowercase.\"\"” text_v1= text.lower()\n",
      "\n",
      "The content of thr paragraph28is :return  re.sub(r”[’a-z\\s]\",  ””,  text)\n",
      "\n",
      "The content of thr paragraph29is :tokenize_text(text):\n",
      "\n",
      "The content of thr paragraph30is :““”Tokenize the text.\"\"” return text.split()\n",
      "\n",
      "The content of thr paragraph31is :remove_stopwords(tokens):\n",
      "\n",
      "The content of thr paragraph32is :“\"\"Remove common stopwords. ”””\n",
      "\n",
      "The content of thr paragraph33is :return [word for word in tokens if word not in stopwords.words(\"english”)]\n",
      "\n",
      "The content of thr paragraph34is :lemmatize_tokens(tokens):\n",
      "\n",
      "The content of thr paragraph35is :“””Lemmatize tokens to their base form. ””” lemmatizer_v1= WordNetLemmatizer()\n",
      "\n",
      "The content of thr paragraph36is :return [lemmatizer.lemmatize(word) for word in tokens]\n",
      "\n",
      "The content of thr paragraph37is :# (v1)  AppLy pneproces si ng yunc I i ons\n",
      "\n",
      "The content of thr paragraph38is :data[”Cleaned_Text”]\tdata[”Review”].apply(clean_text) data[”Tokens“]\tdata[\"Cleaned_Text”].apply(tokenize_text) data[”Tokens_No_Stopwords\"]\tdata[\"Tokens”].apply(remove_stopwords)\n",
      "\n",
      "The content of thr paragraph39is :data[”Lemmatized_Tokens\"]\tdata[\"Tokens_No_Stopwords\"].apply(lemmatize_tokens  data[”Processed_Text\"]\tdata[”Lemmatized_Tokens”].apply(lambda  tokens:\t”.jo:\n",
      "\n",
      "The content of thr paragraph40is :In [4]:\n",
      "\n",
      "The content of thr paragraph41is :\n",
      "# (v1)  3. Vi suaL i z i ng Dataset and Key Stats print(”Full Dataset with Preprocessing Steps:”) print(data)\n",
      "\n",
      "The content of thr paragraph42is :Full Dataset with Preprocessing Steps:\n",
      "\n",
      "The content of thr paragraph43is :Review Sentiment\n",
      "\n",
      "The content of thr paragraph44is :Cleaned_Text  \n",
      "\n",
      "The content of thr paragraph45is :This  product  is  fine.\tneutral\n",
      "\n",
      "The content of thr paragraph46is :This product is hate. negative\n",
      "\n",
      "The content of thr paragraph47is :This product is love. positive\n",
      "\n",
      "The content of thr paragraph48is :This product is okay.\tneutral\n",
      "\n",
      "The content of thr paragraph49is :This product is poor. negative\n",
      "\n",
      "The content of thr paragraph50is :\n",
      "this product is fine this product is hate this product is love this product is okay this product is poor\n",
      "\n",
      "The content of thr paragraph51is :This  product  is  fantastic.\n",
      "\n",
      "The content of thr paragraph52is :This product is satisfactory.\n",
      "\n",
      "The content of thr paragraph53is :This product is mediocre.\n",
      "\n",
      "The content of thr paragraph54is :This product is great.\n",
      "\n",
      "The content of thr paragraph55is :This  product  is  great.\n",
      "\n",
      "The content of thr paragraph56is :\n",
      "positive  this  product  is  fantastic  neutral  this  product  is  satisfactory  neutral  this  product  is  mediocre  positive        this  product  is  great\n",
      "\n",
      "The content of thr paragraph57is :positive        this  product  is  great\n",
      "\n",
      "The content of thr paragraph58is :Tokens\n",
      "\n",
      "The content of thr paragraph59is :[this, product, is, fine]\n",
      "\n",
      "The content of thr paragraph60is :[this, product, is, hate]\n",
      "\n",
      "The content of thr paragraph61is :[this, product, is, love]\n",
      "\n",
      "The content of thr paragraph62is :[this, product, is, okay]\n",
      "\n",
      "The content of thr paragraph63is :[this, product, is, poor]\n",
      "\n",
      "The content of thr paragraph64is :\n",
      "Tokens_No_Stopwords  \\  [product,  fine]  [product,  hate]  [product,  love]  [product,  okay]  [product,  poor]\n",
      "\n",
      "The content of thr paragraph65is :[this, product, is, fantastic]\t[product, fantastic]\n",
      "\n",
      "The content of thr paragraph66is :[this, product, is, satisfactory] [product, satisfactory]\n",
      "\n",
      "The content of thr paragraph67is :[this, product, is, mediocre]\t[product, mediocre]\n",
      "\n",
      "The content of thr paragraph68is :[this, product, is, great]\t[product, great]\n",
      "\n",
      "The content of thr paragraph69is :[this, product, is, great]\t[product, great]\n",
      "\n",
      "The content of thr paragraph70is :Lemmatized_Tokens\n",
      "\n",
      "The content of thr paragraph71is :[product, fine]\n",
      "\n",
      "The content of thr paragraph72is :[product,  hate]\n",
      "\n",
      "The content of thr paragraph73is :[product, love]\n",
      "\n",
      "The content of thr paragraph74is :[product,  okay]\n",
      "\n",
      "The content of thr paragraph75is :[product, poor]\n",
      "\n",
      "The content of thr paragraph76is :\n",
      "Processed_Text product fine product hate product love product okay product poor\n",
      "\n",
      "The content of thr paragraph77is :[product,  fantastic]\tproduct  fantastic\n",
      "\n",
      "The content of thr paragraph78is :[product, satisfactory] product satisfactory\n",
      "\n",
      "The content of thr paragraph79is :[product, mediocre]\tproduct mediocre\n",
      "\n",
      "The content of thr paragraph80is :[product, great]\tproduct great\n",
      "\n",
      "The content of thr paragraph81is :[product, great]\tproduct great [1000 rows x 7 columns]\n",
      "\n",
      "The content of thr paragraph82is :In\t[5] :\n",
      "\n",
      "The content of thr paragraph83is :In [6]:\n",
      "\n",
      "The content of thr paragraph84is :In [8] :\n",
      "\n",
      "The content of thr paragraph85is :In [10]:\n",
      "\n",
      "The content of thr paragraph86is :\n",
      "# (v1)  5.  NodeL Deve Lopment & EvaL uat i on\n",
      "\n",
      "The content of thr paragraph87is :models\t(\n",
      "\n",
      "The content of thr paragraph88is :“Random Forest” : RandomForestClassifier(n_estimators_v1=100, random_state=42) “Logistic Regression” : LogisticRegression(random_state=42),\n",
      "\n",
      "The content of thr paragraph89is :“SVM” : SVC(kernel_v1=\"linear”, random_state=42)\n",
      "\n",
      "The content of thr paragraph90is :best_model_v1= None best_accuracy = 0\n",
      "\n",
      "The content of thr paragraph91is :print(\"Model Performance Evaluation: \")\n",
      "\n",
      "The content of thr paragraph92is :# (v1)  EvoLUote nodeLs Ond select the best one for\tmode1_name, model 1n mode1s.1tems(): model.fit(X_train_tfidf, y_train)\n",
      "\n",
      "The content of thr paragraph93is :y_pred_v1= model.predict(X_test_tfidf)\n",
      "\n",
      "The content of thr paragraph94is :# (v1)  Netn! cs caL cuL ation\n",
      "\n",
      "The content of thr paragraph95is :accuracy_v1= accuracy_score(y_test, y_pred)\n",
      "\n",
      "The content of thr paragraph96is :precision\tprecision_score(y_test, y_pred, average_v1=\"weighted”) recall = recall_score(y_test, y_pred, average=”weighted\")\n",
      "\n",
      "The content of thr paragraph97is :f1_v1= f1_score(y_test, y_pred, average=\"weighted\")\n",
      "\n",
      "The content of thr paragraph98is :print(f\"\\n{model_name} Performance:\") print(f\"Accuracy: {accuracy * 100:.2f}%“) print(f\"Precision: {precision:.2f}”) print(f\"Recall: {recall:.2f}\")\n",
      "\n",
      "The content of thr paragraph99is :print(f\"F1-Score: {f1:.2f}”)\n",
      "\n",
      "The content of thr paragraph100is :ii Trach  I:he best: code L\n",
      "\n",
      "The content of thr paragraph101is :if accuracy > best_accuracy: best_accuracy_v1= accuracy best_model = model\n",
      "\n",
      "The content of thr paragraph102is :Model Performance Evaluation:\n",
      "\n",
      "The content of thr paragraph103is :Random  Forest  Performance:  Accuracy:  100.00%\n",
      "\n",
      "The content of thr paragraph104is :Precision: 1.00\n",
      "\n",
      "The content of thr paragraph105is :Recall: 1.00\n",
      "\n",
      "The content of thr paragraph106is :F1-Score: 1.00\n",
      "\n",
      "The content of thr paragraph107is :Logistic Regression Performance: Accuracy: 100.00%\n",
      "\n",
      "The content of thr paragraph108is :Precision:  1.00\n",
      "\n",
      "The content of thr paragraph109is :Recall: 1.00\n",
      "\n",
      "The content of thr paragraph110is :F1-Score:  1.00\n",
      "\n",
      "The content of thr paragraph111is :SVM Performance:\n",
      "\n",
      "The content of thr paragraph112is :Accuracy: 100.00%\n",
      "\n",
      "The content of thr paragraph113is :Precision:  1.00\n",
      "\n",
      "The content of thr paragraph114is :Recall: 1.00\n",
      "\n",
      "The content of thr paragraph115is :F1-Score:  1.00\n",
      "\n",
      "The content of thr paragraph116is :In [11]:\n",
      "\n",
      "The content of thr paragraph117is :Best Model: RandomForestClassifier(random_state_v1=42)\n",
      "\n",
      "The content of thr paragraph118is :In [12] :  # 7.  Di spL ay Dataset with Predicted Sentiment\n",
      "\n",
      "The content of thr paragraph119is :data[”Predicted_Sentiment”]\tbest_model.predict(vectorizer.transform(data[“Pro\n",
      "\n",
      "The content of thr paragraph120is :# (v1)  FinaL Dataset  with predi c I i ons\n",
      "\n",
      "The content of thr paragraph121is :print(”\\nFinal Dataset with Predicted Sentiments: ”) print(data)\n",
      "\n",
      "The content of thr paragraph122is :# (v1)  Save the  fi naL  dataset with pnedi ctions data.to_csv(\"final_processed_dataset_with_predictions.csv”, index=False) print(”Final processed dataset with predictions has been saved successfully! \")\n",
      "\n",
      "The content of thr paragraph123is :1/13/25,  1:58  PM\tUntitled83  -  Jupyter  Notebook\n",
      "\n",
      "The content of thr paragraph124is :Final Dataset with Predicted Sentiments:\n",
      "\n",
      "The content of thr paragraph125is :Review Sentiment\n",
      "\n",
      "The content of thr paragraph126is :This product is fine.\tneutral\n",
      "\n",
      "The content of thr paragraph127is :This  product  is  hate.  negative\n",
      "\n",
      "The content of thr paragraph128is :This product is love. positive\n",
      "\n",
      "The content of thr paragraph129is :This product is okay.\tneutral\n",
      "\n",
      "The content of thr paragraph130is :This  product  is  poor.  negative\n",
      "\n",
      "The content of thr paragraph131is :Cleaned_Text  \\  this  product  is  fine  this  product  is  hate  this  product  is  love  this  product  is  okay  this  product  is  poor\n",
      "\n",
      "The content of thr paragraph132is :This product is fantastic.\n",
      "\n",
      "The content of thr paragraph133is :This product is satisfactory.\n",
      "\n",
      "The content of thr paragraph134is :This product is mediocre.\n",
      "\n",
      "The content of thr paragraph135is :This product is great.\n",
      "\n",
      "The content of thr paragraph136is :This product is great.\n",
      "\n",
      "The content of thr paragraph137is :\n",
      "positive neutral neutral positive positive\n",
      "\n",
      "The content of thr paragraph138is :\n",
      "this product is fantastic this product is satisfactory this product is mediocre this product is great\n",
      "\n",
      "The content of thr paragraph139is :this product is great\n",
      "\n",
      "The content of thr paragraph140is :[this, product, is, fantastic]\n",
      "\n",
      "The content of thr paragraph141is :[this, product, is, satisfactory]\n",
      "\n",
      "The content of thr paragraph142is :[this,  product,  is,  mediocre]\n",
      "\n",
      "The content of thr paragraph143is :[this,  product,  is,  great]\n",
      "\n",
      "The content of thr paragraph144is :[this,  product,  is,  great]\n",
      "\n",
      "The content of thr paragraph145is :\n",
      "[product, fantastic] [product, satisfactory] [product, mediocre] [product, great] [product, great]\n",
      "\n",
      "The content of thr paragraph146is :Lemmatized_Tokens\n",
      "\n",
      "The content of thr paragraph147is :[product,  fine]\n",
      "\n",
      "The content of thr paragraph148is :[product, hate]\n",
      "\n",
      "The content of thr paragraph149is :[product, love]\n",
      "\n",
      "The content of thr paragraph150is :[product, okay]\n",
      "\n",
      "The content of thr paragraph151is :[product,  poor]\n",
      "\n",
      "The content of thr paragraph152is :\n",
      "Processed_Text  product  fine  product  hate  product  love  product  okay  product  poor\n",
      "\n",
      "The content of thr paragraph153is :\n",
      "Text_Length \\\n",
      "\n",
      "The content of thr paragraph154is :12\n",
      "\n",
      "The content of thr paragraph155is :12\n",
      "\n",
      "The content of thr paragraph156is :12\n",
      "\n",
      "The content of thr paragraph157is :12\n",
      "\n",
      "The content of thr paragraph158is :12\n",
      "\n",
      "The content of thr paragraph159is :[product, fantastic]\tproduct fantastic\t17\n",
      "\n",
      "The content of thr paragraph160is :[product, satisfactory] product satisfactory\t20\n",
      "\n",
      "The content of thr paragraph161is :[product, mediocre]\tproduct mediocre\t16\n",
      "\n",
      "The content of thr paragraph162is :[product,  great]\tproduct  great\t13\n",
      "\n",
      "The content of thr paragraph163is :[product, great]\tproduct great\t13\n",
      "\n",
      "The content of thr paragraph164is :Predicted_Sentiment\n",
      "\n",
      "The content of thr paragraph165is :neutral\n",
      "\n",
      "The content of thr paragraph166is :negative\n",
      "\n",
      "The content of thr paragraph167is :positive\n",
      "\n",
      "The content of thr paragraph168is :neutral\n",
      "\n",
      "The content of thr paragraph169is :negative\n",
      "\n",
      "The content of thr paragraph170is :positive\n",
      "\n",
      "The content of thr paragraph171is :neutral\n",
      "\n",
      "The content of thr paragraph172is :neutral\n",
      "\n",
      "The content of thr paragraph173is :positive\n",
      "\n",
      "The content of thr paragraph174is :positive\n",
      "\n",
      "The content of thr paragraph175is :[1000 rows x 9 columns]\n",
      "\n",
      "The content of thr paragraph176is :Final processed dataset with predictions has been saved successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(document.paragraphs)):\n",
    "    print(\"The content of thr paragraph\"+ str(i)+\"is :\"+document.paragraphs[i].text+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcde063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "152b7d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d016686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib2\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70d588be",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=urllib2.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "html_doc=response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "731dc2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\" dir=\"ltr\" lang=\"en\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <title>\n",
      "   Natural language processing - Wikipedia\n",
      "  </title>\n",
      "  <script>\n",
      "   (function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\\w+$|[^\\w-]+/g,'')+'-clientpref-\\\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\n",
      "\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"bb8cccbb-931f-4281-aaf8-3aff8c9effb8\",\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Natural_language_processing\",\"wgTitle\":\"Natural language processing\",\"wgCurRevisionId\":1274942014,\"wgRevisionId\":1274942014,\"wgArticleId\":21652,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"All accuracy disputes\",\"Accuracy disputes from December 2013\",\"Harv and Sfn no-target errors\",\"CS1 errors: periodical ignored\",\"CS1 maint: location\",\"Articles with short description\",\"Short description is different from Wikidata\",\"Articles needing additional references from May 2024\",\"All articles needing additional references\",\"All articles with unsourced statements\",\"Articles with unsourced statements from May 2024\",\"Commons category link from Wikidata\",\n",
      "\"Natural language processing\",\"Computational fields of study\",\"Computational linguistics\",\"Speech recognition\"],\"wgPageViewLanguage\":\"en\",\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgRelevantPageName\":\"Natural_language_processing\",\"wgRelevantArticleId\":21652,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgNoticeProject\":\"wikipedia\",\"wgCiteReferencePreviewsActive\":false,\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1}}},\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsFlags\":0,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\"},\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"watchlist\":true,\"tagline\":false,\"nearby\":true},\"wgWMESchemaEditAttemptStepOversample\":false,\"wgWMEPageLength\":60000,\"wgEditSubmitButtonLabelPublish\":true,\"wgULSPosition\":\"interlanguage\",\"wgULSisCompactLinksEnabled\":false,\"wgVector2022LanguageInHeader\":true,\n",
      "\"wgULSisLanguageSelectorEmpty\":false,\"wgWikibaseItemId\":\"Q30642\",\"wgCheckUserClientHintsHeadersJsApi\":[\"brands\",\"architecture\",\"bitness\",\"fullVersionList\",\"mobile\",\"model\",\"platform\",\"platformVersion\"],\"GEHomepageSuggestedEditsEnableTopics\":true,\"wgGETopicsMatchModeEnabled\":false,\"wgGEStructuredTaskRejectionReasonTextInputEnabled\":false,\"wgGELevelingUpEnabledForUser\":false};RLSTATE={\"ext.globalCssJs.user.styles\":\"ready\",\"site.styles\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"user\":\"ready\",\"user.options\":\"loading\",\"ext.cite.styles\":\"ready\",\"ext.math.styles\":\"ready\",\"skins.vector.search.codex.styles\":\"ready\",\"skins.vector.styles\":\"ready\",\"skins.vector.icons\":\"ready\",\"jquery.makeCollapsible.styles\":\"ready\",\"ext.wikimediamessages.styles\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"wikibase.client.init\":\"ready\",\"ext.wikimediaBadges\":\"ready\"};RLPAGEMODULES=[\"ext.cite.ux-enhancements\",\"ext.scribunto.logs\",\"site\",\n",
      "\"mediawiki.page.ready\",\"jquery.makeCollapsible\",\"mediawiki.toc\",\"skins.vector.js\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.switcher\",\"ext.urlShortener.toolbar\",\"ext.centralauth.centralautologin\",\"mmv.bootstrap\",\"ext.popups\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visu\n"
     ]
    }
   ],
   "source": [
    "soup=BeautifulSoup(html_doc,'html.parser')\n",
    "\n",
    "strhtm=soup.prettify()\n",
    "\n",
    "print(strhtm[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696c652d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
